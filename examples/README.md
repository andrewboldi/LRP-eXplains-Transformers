# Example Scripts for LRP

Welcome to the `examples/` folder!  
This directory contains example scripts and outputs to help you apply **Layer-wise Relevance Propagation (LRP)** to Transformer models using PyTorch.


## üîß Recommended Usage: `examples/*`

For practical, real-world applications, we recommend using the scripts found directly under `examples/` (e.g., `examples/quantized_llama.py`).  
These use an **efficient implementation** of LRP that:

- Leverages PyTorch's automatic differentiation,
- Follows an Input√óGradient-style framework,
- Is much faster and cleaner than earlier versions.

This version is inspired by the method described in:  
[A Close Look at Decomposition-based XAI-Methods for Transformer Language Models ‚Äì Leila Arras et al.](https://arxiv.org/abs/2502.15886)

**Tested with:**

```text
transformers==4.52.4
torch==2.6.0
python==3.11
```

## üå°Ô∏è Heatmap Outputs: `examples/heatmaps`

The `examples/heatmaps/` folder contains attribution heatmaps generated by the example scripts. These visualizations are produced via LaTeX; therefore, you should have `pdflatex` or, preferably, `xelatex` installed to generate the heatmaps.
These visualizations show which input features (tokens) contributed most to the model‚Äôs predictions.


## üìö Paper Reproducibility: `examples/paper`

The `examples/paper/` directory contains the original scripts used to produce figures for the paper.  
These scripts use a more **explicit but inefficient** implementation of LRP. While not ideal for deployment, they are useful for understanding the underlying mathematical concepts. In addition, this code is outdated and only works for older versions of PyTorch and transformers.

**Tested with:**

```text
transformers==4.46.2
torch==2.1.0
python==3.11
```

## üìñ Documentation

For a deeper explanation of the LRP framework and available options, please refer to the full [Documentation](https://lxt.readthedocs.io/).