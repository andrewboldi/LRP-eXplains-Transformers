# Layer-wise Relevance Propagation eXplains Transformers (LXT)

Welcome to the Documentation! 
LXT contains the Layer-wise Relevance Propagation (LRP) implementation extended to handle attention layers in Large Language Models (LLMs) and Vision Transformers (ViTs).

Attention-aware LRP (AttnLRP) **outperforms** gradient- and perturbation-based methods, provides faithful attributions for the **entirety** of a black-box transformer model while scaling in computational complexitiy O(1) and memory requirements O(âˆšN).


```{eval-rst} 
.. important::
   Project is under active development! Feature visualization and perturbation experiments are still missing.
```

::::{grid} 2

:::{card} ðŸš€ Quickstart
:link: quickstart
:link-type: ref
Examples for ðŸ¤— (Tiny)LLaMA 3/2, Mixtral 8x7b, CLIP ViT
:::

:::{card} ðŸ’¡ LXT Drop-In Replacements
:link: drop_in_replacement
:link-type: ref
Modify your custom source-code
:::

:::{card} ðŸ¤– On-the-fly Modifications
:link: on_the_fly
:link-type: ref
Modify your code during runtime via LXT Composites and Symbolic Graph Tracing
:::

:::{card} ðŸ”Ž Latent Feature Attribution & Visualization
:link: latent_feature_attribution
:link-type: ref
Trace the internal reasoning process of a transformer
:::

::::


## Installation

To install directly from PyPI using pip, write:

```shell
$ pip install lxt
```

To reproduce the experiments from the paper, install from a manually cloned repository: 

```shell
$ git clone https://github.com/rachtibat/LRP-for-Transformers
$ pip install ./lxt
```

## License
This project is licensed under the BSD-3 Clause License, which means that LRP is a patented technology that can only be used free of charge for personal and scientific purposes.

## Paper
For understanding the math behind it, take a look at the [paper](https://arxiv.org/abs/2402.05602)!

```latex
@article{achtibat2024attnlrp,
  title={AttnLRP: Attention-aware Layer-wise Relevance Propagation for Transformers},
  author={Achtibat, Reduan and Hatefi, Sayed Mohammad Vakilzadeh and Dreyer, Maximilian and Jain, Aakriti and Wiegand, Thomas and Lapuschkin, Sebastian and Samek, Wojciech},
  journal={arXiv preprint arXiv:2402.05602},
  year={2024}
}
```


## Table of Content

```{eval-rst} 
.. toctree::
   quickstart
   under-the-hood
   drop-in-replacement
   on-the-fly
   debugging
   :maxdepth: 3
   :caption: LXT Usage:

.. toctree::
   feature-visualization
   :maxdepth: 3
   :caption: Experiments:

.. toctree::
   core
   functional
   modules
   rules
   :maxdepth: 2
   :caption: API Documentation:
```

